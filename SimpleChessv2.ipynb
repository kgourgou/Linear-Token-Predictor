{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pzl1_oBuEjQ1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from prodigyopt import Prodigy\n",
    "\n",
    "from IPython.display import HTML\n",
    "import re\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "from tokenizers.pre_tokenizers import Sequence, Split\n",
    "from tokenizers.normalizers import Replace, Sequence as NormSequence\n",
    "\n",
    "\n",
    "# check if we are in a colab environment\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "from IPython.display import display, SVG, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import chess\n",
    "import chess.svg\n",
    "\n",
    "\n",
    "subset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dAUcYOvi3ik6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 101/101 [00:00<00:00, 11125.77 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to data/chess_dataset.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and subset the dataset\n",
    "\n",
    "TRAIN_SIZE = 100\n",
    "\n",
    "def clean_chess_moves(movetext):\n",
    "    # Remove move numbers (e.g., '1.', '2.') using regex\n",
    "    movetext = re.sub(r'\\d+\\.', '', movetext)\n",
    "\n",
    "    # Remove final score (e.g., '1-0', '0-1', or '1/2-1/2')\n",
    "    movetext = re.sub(r'\\b(1-0|0-1|1/2-1/2)\\b', '', movetext)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    movetext = ' '.join(movetext.split())\n",
    "\n",
    "    return movetext\n",
    "\n",
    "if len(subset) == 0:\n",
    "  ds = load_dataset(\"Lichess/standard-chess-games\", split='train', streaming=True)\n",
    "  for idx, example in enumerate(tqdm(ds, desc=\"Loading dataset\")):\n",
    "      subset.append(example['movetext'])\n",
    "      if idx == TRAIN_SIZE:\n",
    "          break\n",
    "\n",
    "chess_dataset = HFDataset.from_dict({'text': subset})\n",
    "chess_dataset = chess_dataset.map(lambda x: {'text' : clean_chess_moves(x['text'])})\n",
    "\n",
    "# save to file \n",
    "\n",
    "save_path = \"data/chess_dataset.txt\"\n",
    "with open(save_path, \"w\") as f:\n",
    "    for movetext in chess_dataset: \n",
    "        f.write(movetext['text'])\n",
    "    \n",
    "print(f\"Dataset saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "V5T1bBmxEUkW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps' if torch.mps.is_available() else device\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mMtJd8up4nNb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e4', 'e6', 'd4', 'b6', 'a3', 'Bb7', '[UNK]', 'O-O']\n",
      "Vocabulary size: 1001\n",
      "['Rcd1', 'Ng5+', 'Rfa8', 'bxc6', 'Bxc6', 'Rxf4', 'Ndxf2', 'Bxg4', 'cxd6', 'Qxf6+', 'Rxb5', 'Re7', 'Qe6', 'Qh7#e4', 'Kxf2', 'Qxa8', 'Rxh7', 'Kxb3', 'Rhb1', 'Rf1+', 'Rxc1+', 'axb6', 'Nxf1', 'Rad1+', 'Rxc3+', 'h2', 'Rxc6', 'Kc5e4', 'Nd7+e4', 'dxc6', 'Rf4#e4', 'fxe4', 'Nc1', 'Rhe1', 'Nxg6', 'Bc4', 'Nec1', 'Bxe6+', 'Rxc2+', 'Bg2', 'Bxh8e4', 'Rxa3', 'fxg3+', 'Rc2+', 'Bd7', 'Kb7', 'Kxe3', 'Qxa1+', 'Rd2+', 'Qxh1', 'Qg4+', 'Rxf7', 'Bxe6', 'Qxd5', 'Nf6', 'Qa4+', 'Bb4+', 'Qe4e6', 'Nh5', 'Nd5+', 'Qe5', 'Rbf8', 'Rc2#e4', 'Rxb6', 'Nbc6', 'Nfxd5', 'Qxc6', 'Rxe5', 'Nc1d3', 'Rxh3', 'h7', 'Nxe6', 'Nb5', 'Bf1', 'Rad1', 'Rb8+', 'Rbb3', 'Bxd2', 'bxa4', 'Bxd4+', 'Rdf8', 'Nxd2', 'Kc2', 'Bf2#e4', 'Kh7', 'Rh5', 'cxb3', 'Be7', 'cxb6', 'Re8+', 'Rc1+', 'Qe8e7+', 'Nh4', 'Qd3', 'Nxd3', 'Nbd2', 'Qxe4+', 'Bb5', 'Qd1#e4', 'Qg2+']\n"
     ]
    }
   ],
   "source": [
    "# let's train a smaller tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))  \n",
    "\n",
    "# Custom normalizer to ensure + and # are attached to moves\n",
    "normalizer = NormSequence([\n",
    "    Replace(r\"(\\S+)\\s+([+#])\", r\"\\1\\2\")  \n",
    "])\n",
    "tokenizer.normalizer = normalizer\n",
    "\n",
    "# Custom pre-tokenizer sequence\n",
    "pre_tokenizer_sequence = Sequence([\n",
    "    # First split by whitespace\n",
    "    Split(pattern=\" \", behavior=\"removed\"),\n",
    "    # Then merge + and # with previous token if they were split\n",
    "    Split(pattern=r\"([+#])\", behavior=\"merged_with_previous\")\n",
    "])\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizer_sequence\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    vocab_size=1000,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    min_frequency=1,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Train tokenizer on the chess-dataset\n",
    "tokenizer.train(files=[\"data/chess_dataset.txt\"], trainer=trainer)\n",
    "\n",
    "# Save the trained tokenizer to a file (not a directory)\n",
    "tokenizer.save(\"data/custom_chess_tokenizer.json\")\n",
    "\n",
    "# 2. Wrap the tokenizer in a Hugging Face-compatible wrapper\n",
    "hf_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "hf_tokenizer.save_pretrained(\"data/custom_chess_tokenizer\")  # Now save to a directory for Hugging Face compatibility\n",
    "\n",
    "# 3. Load and use the tokenizer with AutoTokenizer\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\"data/custom_chess_tokenizer\")\n",
    "trained_tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token' : '[EOS]'})\n",
    "new_tokens = [\"O-O\", \"O-O-O\"]\n",
    "trained_tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Test the trained tokenizer\n",
    "test_text = \"e4 e6 d4 b6 a3 Bb7 Nc3+ O-O\"\n",
    "\n",
    "\n",
    "tokens = trained_tokenizer.tokenize(test_text)\n",
    "print(tokens)\n",
    "\n",
    "# print vocabulary\n",
    "vocab = trained_tokenizer.get_vocab()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(list(vocab.keys())[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wf4KixFy30QL"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LinearTokenPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    This model is a simple linear model that predicts the next token in a sequence.\n",
    "\n",
    "    Originally three layers:\n",
    "    1. An input embedding layer with dimension $d=256$\n",
    "    2. A linear layer mapping dxT to dxT (with standard masking for the next tokens during training)\n",
    "    3. An output embedding layer mapping vectors of dimension $d$ to the vocabulary.\n",
    "\n",
    "    But I also added a layer norm between 2 and 3 to help a bit.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, vocab_size:int , context_size: int=64, d:int =256, device:str = 'cuda'):\n",
    "        super(LinearTokenPredictor, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d = d\n",
    "        self.context_size = context_size\n",
    "        linear_dim = context_size*d\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d)\n",
    "        self.linear = nn.Parameter(torch.randn(linear_dim, linear_dim) / np.sqrt(linear_dim))\n",
    "        self.output = nn.Linear(d, vocab_size, bias=False)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(linear_dim)\n",
    "\n",
    "        self.mask = self.create_mask(d, context_size).T.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        For training, we use a causal mask to limit the\n",
    "        linear layer to only consider previous tokens.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "\n",
    "\n",
    "        # map from batch x seq x d to batch x (seq*d)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = x @(self.linear*self.mask)\n",
    "        x = self.layer_norm(x) # small addition\n",
    "\n",
    "        # map back to batch x seq x d\n",
    "        x = x.view(x.size(0), -1, self.d)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, token_list: torch.Tensor | str, n:int = 1,\n",
    "                 return_html=True, html_font_size:int=12):\n",
    "        \"\"\"\n",
    "        Given a tensor of token-ids, generate n tokens.\n",
    "        \"\"\"\n",
    "        if isinstance(token_list, str):\n",
    "          token_list = self.tokenizer(token_list)\n",
    "          token_list = token_list['input_ids']\n",
    "        else:\n",
    "          token_list = token_list.tolist()\n",
    "\n",
    "        len_list  = len(token_list)\n",
    "\n",
    "        if len_list < self.context_size:\n",
    "            token_list = token_list + [self.tokenizer.eos_token_id] * (self.context_size - len(token_list))\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "          for i in range(n):\n",
    "              # keep token list within context by using the last T tokens\n",
    "              x = torch.tensor(\n",
    "                  token_list[-self.context_size:]\n",
    "                  ).unsqueeze(0).to(self.device)\n",
    "              logits = self.forward(x)\n",
    "\n",
    "              if len_list + i - 1 < self.context_size:\n",
    "                curr_token_index = len_list + i - 1 # -1 because the first logit corresponds to P(token_1|token_0)\n",
    "              else:\n",
    "                curr_token_index = -1\n",
    "\n",
    "              logits = logits[:,curr_token_index]\n",
    "              tok = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "              if (len_list + i) < self.context_size:\n",
    "                token_list[len_list + i] = tok\n",
    "              else:\n",
    "                token_list.append(tok)\n",
    "\n",
    "\n",
    "        answer = self.tokenizer.decode(token_list[len_list:], skip_special_tokens=False)\n",
    "        if return_html:\n",
    "          prompt = self.tokenizer.decode(token_list[:len_list], skip_special_tokens=True)\n",
    "          return (f'<span style=\"font-size: {html_font_size}px\"> <span style=\"color: green;\">'\n",
    "                  + prompt + '</span> '\n",
    "                  + answer + '</span></br>')\n",
    "        else:\n",
    "          return answer\n",
    "\n",
    "\n",
    "    def create_mask(self, d:int , T:int):\n",
    "      mask = np.tril(np.ones((T, T)))\n",
    "      expanded_mask = np.kron(mask, np.ones((d, d)))\n",
    "      expanded_mask = torch.tensor(expanded_mask, dtype=torch.float32)\n",
    "      return expanded_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oSvX6RFv37ld"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 101/101 [00:00<00:00, 2817.68 examples/s]\n",
      "Map: 100%|██████████| 101/101 [00:00<00:00, 2181.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_size = TRAIN_SIZE\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "\n",
    "\n",
    "def truncate(text, max_length):\n",
    "    \"\"\"\n",
    "    Randomly shuffle the start of the text to create different starting points,\n",
    "    and truncate to max_length if necessary.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "\n",
    "    return ' '.join(tokens[:max_length])\n",
    "\n",
    "\n",
    "# TODO remove numbers from text, only keep moves.\n",
    "\n",
    "processed_ds = chess_dataset.map(lambda x:\n",
    "                                      {'text': truncate(x['text'], MAX_LENGTH)})\n",
    "\n",
    "train_set = processed_ds.map(lambda x: trained_tokenizer(x['text'],\n",
    "                                                 padding='max_length',\n",
    "                                                 max_length=MAX_LENGTH,\n",
    "                                                 truncation=True,\n",
    "                                                 return_tensors='pt'), batched=True)\n",
    "\n",
    "train_set.set_format(type='torch', columns=['input_ids'])\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                           batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "t2x-mTLgE_Iy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = trained_tokenizer.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "context_size = MAX_LENGTH -1\n",
    "d = 32 # if train_size is small, you may want to keep this small as well\n",
    "\n",
    "model = None\n",
    "model = LinearTokenPredictor(trained_tokenizer,\n",
    "                             len(trained_tokenizer),\n",
    "                             context_size=context_size,\n",
    "                             d=d, device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oXiDKJSmFfRK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter count: 16588288\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:05<02:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 7.003628730773926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:05<00:14,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss: 4.917160511016846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:06<00:06, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 loss: 0.34555402398109436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:07<00:04, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 loss: 0.06236353516578674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:08<00:03, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 loss: 0.037004031240940094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:08<00:03, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 loss: 0.023584822192788124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:09<00:02, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 loss: 0.016063058748841286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [00:10<00:01, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 loss: 0.011385445483028889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:10<00:01, 14.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 loss: 0.008320871740579605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [00:11<00:00, 14.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 loss: 0.0062261163257062435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:12<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "print(f'parameter count: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = trained_tokenizer.pad_token_id)\n",
    "optimizer = Prodigy(model.parameters())\n",
    "\n",
    "loss_tracking = []\n",
    "\n",
    "EPOCHS = 100\n",
    "TOL_EARLY_STOP = 0.00001\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "\n",
    "        inputs = input_ids[:, :-1]  # All tokens except the last one\n",
    "        targets = input_ids[:, 1:] # All tokens except the first one\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs).permute(0,2,1)\n",
    "\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_tracking.append(loss.item())\n",
    "\n",
    "    last_loss = sum(loss_tracking)/len(loss_tracking)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(f'Epoch {epoch} loss: {last_loss}')\n",
    "\n",
    "    if last_loss < TOL_EARLY_STOP:\n",
    "      break\n",
    "\n",
    "    loss_tracking = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ufgYrboVg4BL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-size: 12px\"> <span style=\"color: green;\">d4 d6</span> e4 Nd7 Bb5 c6 Nd4 e5 d5 Ngf6 Bg5 h6 Bh4 c5 Qf3 a6 a4 Qb6 b3 Qa5+ Nd2 b5 Be2 Be7 Bxf6 Bxf6 Qh3 Bg5 Nf3 Nf6 Qg3 Nxe4 Rd1 Nxg3 fxg3 Be3 Nh4 bxa4 Nf5 Bxf5 bxa4 Bxc2 Ra1 Qxd2+ Kf1 Bd3 Re1 Rb8 Bh4 Bxe6 d5 dxc4 [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS]</span></br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(model.generate(trained_tokenizer.encode(\"d4 d6\",\n",
    "\n",
    "                                                 return_tensors='pt' ).squeeze(),\n",
    "                    n=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "fomJEPYiFneA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e4 e5 Nf3 Nc6 Bc4 Nf6 Nc3 Bc5 a3 Bxf2+ Kxf2 Nd4 d3 Ng4+ Kf1 Qf6 h3 d5 Nxd5 Qe6 [UNK] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-size: 15px\"> <span style=\"color: green;\">e4</span> g6 c4 Bg7 d4 b6 Nf3 c5 d5 Nf6 Nc3 O-O Bd3 e6 d6 Nc6 Nb5 Ba6 Nc7 Bb7 Nxa8 Qxa8 O-O Nd4 Ne5 Rd8 f3 Ne8 Qa4 Bxe5 f4 Bxd6 f5 exf5 exf5 Bxg2 fxg6 Bxf1 [UNK] Kh8 Qc2 Nxc2 Rd1 Nc5 Ne5 Nd3+ Kf1 e5 dxc5 Qh6 Ne3 [UNK] [UNK] [UNK] [UNK] Bxe7 Nxf7+ Kg8 Qxf4 Kf8 Kc4 Qxa6 f3 a6 Bg4 e4 Rc2 [UNK] Kh4 Kg6 Nh3 Nxd5 Kf3 Rxf6 [UNK] Kxf6 Nxc3 Qh4+ Kxh5 Ne8 Kf5 Rf3 d3 [UNK] Qxd4 Qe7 Bxe6 d6 Kf1 Kf8 Kf1 Qxd4 f4 Bb5+ Bg6 [UNK] Rxb7 f4 e3 Kf3 [UNK] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS] [EOS]</span></br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i = 2\n",
    "\n",
    "\n",
    "def parse_chess_moves(game_string):\n",
    "    # Remove move numbers and extra spaces\n",
    "    cleaned_moves = re.sub(r\"\\d+\\.\\s*\", \"\", game_string).strip()\n",
    "    # Split moves into a list\n",
    "    move_list = cleaned_moves.split()\n",
    "    return move_list\n",
    "\n",
    "print(trained_tokenizer.decode(inputs[i]))\n",
    "HTML(model.generate(inputs[i][0:1], n=100, return_html=True, html_font_size=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1_VCZ5Unyd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Chess! You are playing as White. Enter moves in algebraic notation (e.g., e4, Nf3).\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    # Initialize the chess board\n",
    "    board = chess.Board()\n",
    "    tokenizer = trained_tokenizer\n",
    "\n",
    "    def model_move(model, board, tokenizer, n, game):\n",
    "        print(game)\n",
    "        generated_moves = model.generate(game, n=n, return_html=False).split()\n",
    "        print(generated_moves)\n",
    "\n",
    "        for move in generated_moves:\n",
    "            try:\n",
    "                board.push_san(move)\n",
    "                return move\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    # Create a vertical box layout for the game display\n",
    "    game_display = widgets.Output()\n",
    "    controls = widgets.VBox([\n",
    "        widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Type e4, Nf3, etc.',\n",
    "            description='Move:',\n",
    "            disabled=False\n",
    "        ),\n",
    "        widgets.Button(description='Make Move')\n",
    "    ])\n",
    "\n",
    "    # Create the main layout\n",
    "    main_box = widgets.VBox([game_display, controls])\n",
    "\n",
    "    # Game state\n",
    "    game_state = {'game': '', 'is_game_over': False}\n",
    "\n",
    "    def update_display():\n",
    "        with game_display:\n",
    "            clear_output(wait=True)\n",
    "            display(SVG(chess.svg.board(board, size=400)))\n",
    "\n",
    "    def on_button_click(b):\n",
    "        if game_state['is_game_over']:\n",
    "            return\n",
    "\n",
    "        player_move = controls.children[0].value  # Get move from text input\n",
    "\n",
    "        try:\n",
    "            board.push_san(player_move)\n",
    "            game_state['game'] += player_move + ' '\n",
    "\n",
    "            update_display()\n",
    "            print(\"Your move:\", player_move)\n",
    "\n",
    "            if board.is_game_over():\n",
    "                print(\"Game Over!\")\n",
    "                print(\"Result:\", board.result())\n",
    "                game_state['is_game_over'] = True\n",
    "                return\n",
    "\n",
    "            # Model's turn\n",
    "            print(\"Model's turn...\")\n",
    "            model_move_str = model_move(model, board, tokenizer, 1, game_state['game'])\n",
    "\n",
    "            if model_move_str is None:\n",
    "                print(\"Model failed to generate a move. Game over.\")\n",
    "                game_state['is_game_over'] = True\n",
    "                return\n",
    "\n",
    "            game_state['game'] += model_move_str + ' '\n",
    "            print(f\"Model plays: {model_move_str}\")\n",
    "\n",
    "            update_display()\n",
    "\n",
    "            if board.is_game_over():\n",
    "                print(\"Game Over!\")\n",
    "                print(\"Result:\", board.result())\n",
    "                game_state['is_game_over'] = True\n",
    "                return\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid move! Try again.\")\n",
    "\n",
    "        controls.children[0].value = ''  # Clear input field\n",
    "\n",
    "    # Connect the button click event\n",
    "    controls.children[1].on_click(on_button_click)\n",
    "\n",
    "    # Initial display\n",
    "    print(\"Welcome to Chess! You are playing as White. Enter moves in algebraic notation (e.g., e4, Nf3).\")\n",
    "    update_display()\n",
    "\n",
    "    # Show the interface\n",
    "    display(main_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AN0OIOSjMPx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
